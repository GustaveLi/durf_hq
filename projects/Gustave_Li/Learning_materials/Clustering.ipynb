{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0447cd1a",
   "metadata": {},
   "source": [
    "# k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c93467",
   "metadata": {},
   "source": [
    "## Algorithm of k-Means\n",
    "\n",
    "- Given the number of clusters $k$, k-Means first pick $k$ cluster centers (centroids) randomly\n",
    "- k-Means assign the instances to its closest centroid and lable them\n",
    "- It calculates the ***inertia*** of the clustering, which is the sum of squared distances of samples to their closest cluster center\n",
    "- k-Means updates the cluster center and reassigns the instances so as to decrease the inertia\n",
    "- Step 2-4 repeats until the centroids stop moving, which gives the final result\n",
    "- The algorithm is guaranteed to converge in a finite number of steps (usually quite small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1ff9a",
   "metadata": {},
   "source": [
    "## Inertia vs. *silhouette score*\n",
    "- Inertia\n",
    "    - Defined by sum of squared distances of samples to their closest cluster center\n",
    "    - Useful for evaluating the clustering performance when **the number of clusters are not changing**\n",
    "    - Inertia generally **decreases with the number of clusters**, since the instance will be closer when there are more cluster centers, thus it is not a good criteria to find the optimal number of cluster numbers\n",
    "- Silhouette score (Silhouette coefficient)\n",
    "    - Silhouette score is the mean *Silhouette coefficient* over all instances\n",
    "    - Silhouette coefficient is defined as $(b-a)\\, /\\, max(a, b)$, where a is the mean distance to the other instances in the same cluster and b is the mean nearst-cluster distance. The coefficient ranges from -1 to 1\n",
    "    - Silhouette score is a generall benchmark to determine the optimal number of clusters as well as the performance of clustering (not restricted to k-Means only). Silhouette score close to 1 means it is well inside its own cluster while away from other clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71b0e9",
   "metadata": {},
   "source": [
    "## Variants of k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28af5b",
   "metadata": {},
   "source": [
    "### Mini-batch k-Means\n",
    "- Mini-batch k-Means uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function\n",
    "- It is recommended when the datasets are very huge, if the dataset diesn't fit in memory, simply use the numpy `memmap` class\n",
    "- Although mini-batch k-Means is much faster, the inertia is generally worse than the k-Means method, but the quality loss is quite small\n",
    "- Sklearn mini-batch k-Means module: [sklearn.cluster.MiniBatchKMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4535a7a7",
   "metadata": {},
   "source": [
    "### k-Medoid\n",
    "- k-Medoid has very similar algorithm with k-Means, the main difference is the definition to cluster centers. In k-Means, the cluster center is called centroid, which can be every point in the feature space (it may not be a data point). In k-Medoids, the cluster center is medoid, which must be a data instance.\n",
    "- This algorithm is more useful in this project, since our aim is to find triad molecule structure that can represent some specific features.\n",
    "- The k-Medoid algorithm cannot be found in sklearn, use sklearn_extra instead: [sklearn_extra.cluster.KMedoids](https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html#sklearn_extra.cluster.KMedoids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb174f7",
   "metadata": {},
   "source": [
    "## Limitations of k-Means\n",
    "\n",
    "- As the clustering is based on distances of instances and clustering centers, it doesn't behave well when clusters have varying sizes, different densities, or nonspherical shapes\n",
    "- Although the algorithm is guaranteed to converge, it may converge to a suboptimal result, depending on the initial pick of cluster centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db73d1c",
   "metadata": {},
   "source": [
    "## Sklearn k-Means module: [sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf47e1",
   "metadata": {},
   "source": [
    "## Sklearn *silhouette score* module: [sklearn.metrics.silhouette_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e06d059",
   "metadata": {},
   "source": [
    "# *Density-Based Spatial Clustering of Applications with Noise* (DBSCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcbace6",
   "metadata": {},
   "source": [
    "## Algorithm of DBSCAN\n",
    "- Given a distance value $\\epsilon$ (epsilon), DBSCAN counts how many instances are located within $\\epsilon$ from it. This region is called the instance's $\\epsilon-neighborhood$\n",
    "- If an instance has at least $min\\_samples$ instances in its $\\epsilon-neighborhood$ (inluding itself), then it is considered a $core \\ instance$. In other words, core instances are those that are located in dense neighbors.\n",
    "- All instances in the $\\epsilon-neighborhood$ of a core instance belong to the same cluster. This neighborhood may include other core instances, thus, a long sequence of neighboring core instances forms a single cluster.\n",
    "- Any instance that is not a core instance and does not have one in its neighborhood is considered an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59870a4",
   "metadata": {},
   "source": [
    "## Advantages of DBSCAN\n",
    "- DBSCAN works well if all the clusters are dense enough and if they are well separated by low density regions\n",
    "- Unlike k-means, which assumes that clusters are convex shaped, the cluster found by DBSCAN can be any shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39209533",
   "metadata": {},
   "source": [
    "## Limitations of DBSCAN\n",
    "- It may by hard for users to tune the hyperparameters (`eps` and `min_samples`) \n",
    "- DBSCAN may not be able to cluster correctly when the \"denser\" parts in the whole feature space have varying densities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e82815",
   "metadata": {},
   "source": [
    "## Sklearn DBSCAN module: [sklearn.cluster.DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d368cb4d",
   "metadata": {},
   "source": [
    "# *Hierarchical DBSCAN* (HDBSCAN)\n",
    "An improvement to DBSCAN, able to find clusters of varying densities, more intuitive hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e88713",
   "metadata": {},
   "source": [
    "## Algorithm of HDBSCAN\n",
    "- Performs DBSCAN over varying epsilon values and integrates the result to find a clustering that gives the best stability over epsilon\n",
    "- It first finds the dense part of the dataset (very much similar to DBSCAN) as some preliminary clusters\n",
    "- Then it generates the hierarchy of clusters, i.e.**when the distance threshold changes form high to low, which cluster end up merging together and in what order**.\n",
    "- According to the hyperparameters (e.g. `min_cluster_size`, `min_samples`, `cluster_selection_epsilon`), the algorithm decides whether it will merge two preliminary clusters or leave them as separated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8de4d2",
   "metadata": {},
   "source": [
    "## HDBSCAN module: [hdbscan.HDBSCAN](https://hdbscan.readthedocs.io/en/latest/api.html#hdbscan)\n",
    "## Tuning hyperparameters: [Parameter Selection for HDBSCAN](https://hdbscan.readthedocs.io/en/latest/parameter_selection.html#parameter-selection-for-hdbscan)\n",
    "## Straightforward introduction to the algorithm of HDBSCAN: [A gentle introduction to HDBSCAN and density-based clustering](https://towardsdatascience.com/a-gentle-introduction-to-hdbscan-and-density-based-clustering-5fd79329c1e8)\n",
    "Not included in sklearn toolkit, but in a different `hdbscan` module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25aecf7",
   "metadata": {},
   "source": [
    "# *Gaussian mixture model* (GMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ff9af",
   "metadata": {},
   "source": [
    "## Algorithm of GMM\n",
    "- GMM assumes that the instances were generated from a mixture of $k$ Gaussian distributions with parameter unknown(**the number of the Gaussian distributions equals the number of intended clusters**), through the following process:\n",
    "    - One of the $k$ distributions $j$ are chosen for generating one of the $m$ instances, the probability for this distribution $j$ to be chosen is defined by the distribution's (cluster's) weight $\\varphi$\n",
    "    - The location $\\textbf{x}^{(i)}$ instance is sampled randomly from the gaussian distribution with mean ${\\mu}^{(j)}$ and covariance matrix ${\\sum}^{(j)}$. **The mean is the center of the cluster, while the covariance matrix is the size, shape and orientation of the cluster**\n",
    "- The only known value is the location of the instances, the algorithm attempts to figure out all the other parameters ($k$: number of clusters; $\\mu$: center of the cluster; $\\sum$: size, shape and orientation; $\\varphi$: the area coverage of each cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caab662",
   "metadata": {},
   "source": [
    "## `covariance_type` hyperparameter\n",
    "- In order to reduce complexity of the algorithm and save computing resources, you can specify the shape of the clusters (the type of covariance)\n",
    "- Types included in sklearn module:\n",
    "    - `'spherical'`: All clusters must be spherical, but can have different radius\n",
    "    - `'diag'`: Clusters can take any ellipsoidal shape of any size, but its axes must be parallel to the coordinate axes\n",
    "    - `'tied'`: All clusters must have the same ellipsoidal shape, size, and orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce36a14",
   "metadata": {},
   "source": [
    "## Sklearn Gausian mixture module: [sklearn.mixture.GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a195927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
