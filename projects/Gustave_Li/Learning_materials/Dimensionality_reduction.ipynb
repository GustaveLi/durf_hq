{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e7e235",
   "metadata": {},
   "source": [
    "# *Principle Component Analysis* (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1510fc2",
   "metadata": {},
   "source": [
    "## Principle components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fdd346",
   "metadata": {},
   "source": [
    "In a n-dimensional dataset, PCA first identifies one axis that accounts for the **largest amount of variance** in the dataset. Then it identifies the axis, orthogonal to the first axis, that has the largest remaining variance. This process continues until all n axis are found. This process redefines the coordinates of the dataset. The $i^{th}$ axis is called the $i^{th}$ principle component. (The principle components are generated by a technique called *Singular Value Decomposition* (SVD))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc260ee",
   "metadata": {},
   "source": [
    "## PCA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61260425",
   "metadata": {},
   "source": [
    "With the principle components defined, the dimension could be reduced to *d* (user defined) by projecting the dataset onto the hyperplane defined by the **first *d*** principle components (in order to preserve as much varience as possible)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a658ef",
   "metadata": {},
   "source": [
    "## Explained variance ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4208b",
   "metadata": {},
   "source": [
    "- The ratio indicates the proportion of the dataset's varience that lies along each (selected) principle component\n",
    "- It is a benchmark of how well the varience is preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ec5f4",
   "metadata": {},
   "source": [
    "## The `inverse_transform` method and the reconstruction error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab6c53",
   "metadata": {},
   "source": [
    "- PCA are also able to decompress the reduced dataset to a dataset that has the same \\# of dimensions as the original dataset\n",
    "- Since data loss is inevitable for compression, the mean suqared distance between the original data and the reconstructed data is called *reconstruction error*. Reconstrunction evaluated the performance of PCA model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a90d80",
   "metadata": {},
   "source": [
    "## Sklearn class: [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c993ff6",
   "metadata": {},
   "source": [
    "# *Incremental PCA* (IPCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739d94f",
   "metadata": {},
   "source": [
    "## Protential problems in PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51ed50",
   "metadata": {},
   "source": [
    "PCA requires the whole dataset to fit in the memory for the algorithm to run, the computer may not handle it when the dataset is too large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7019af15",
   "metadata": {},
   "source": [
    "## IPCA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa191abc",
   "metadata": {},
   "source": [
    "The IPCA allows you to split the training set into mini-batches and feed the algorithm one a time. Also, you can use the `np.memmap` class to load only the data in need in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025dd3b2",
   "metadata": {},
   "source": [
    "## Sklearn class: [sklearn.decomposition.IncrementalPCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be632c0",
   "metadata": {},
   "source": [
    "## Numpy class: [numpy.memmap](https://numpy.org/doc/stable/reference/generated/numpy.memmap.html?highlight=memmap#numpy.memmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba73c750",
   "metadata": {},
   "source": [
    "# *Kernel PCA* (kPCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be19beb",
   "metadata": {},
   "source": [
    "## kPCA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea859bf",
   "metadata": {},
   "source": [
    "- The kernel functions are applied to the instances and map them into a very high dimentional space (**feature space**), enabling complex nonlinear projections for dimensionality reduction. The linear projection in the high-diomensional feature space corresponds to a complex nonlinear projection in the original space.\n",
    "- It can be used at preserving clusters of instances after projection, or even unrolling datasets that lie close to a twisted manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21450e",
   "metadata": {},
   "source": [
    "## Choosing the optimal kernel model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4875fa3",
   "metadata": {},
   "source": [
    "- We want to find the suitable model with **minimum reconstruction error**. However, kPCA first expanded the dataset to a very high dimension (feature space) so it would be meaningless to compare the reconstructed (high dimensional) dataset to the original dataset. \n",
    "- The solution is to find a point in the original space that would map close to the feature space, which is called the *pre-image* of the reconstruction\n",
    "- When using `sklearn` module, the `inverse_transform` method for kPCA calculates the pre-image automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed0f0a",
   "metadata": {},
   "source": [
    "## Sklearn class: [sklearn.decomposition.KernelPCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e240118",
   "metadata": {},
   "source": [
    "# *Multidimensional Scaling* (MDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad2882d",
   "metadata": {},
   "source": [
    "## Manifold & manifold learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d474854",
   "metadata": {},
   "source": [
    "- Manifold: a *d*-dimensional manifold is a part of an *n*-dimensional space (*d < n*) that locally resembles a *d*-dimensional hyperplane\n",
    "- Manifold learning: dimensionality reduction algorithms that work by modeling the manifold on which the instances lie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19bb2f",
   "metadata": {},
   "source": [
    "## MDS algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebfb86",
   "metadata": {},
   "source": [
    "- MDS finds a d-dimensional (user-defined) space which best preserves the pairwise distances between points in the dataset. It minimizes a cost function quantifying the difference between the pairwise distance as measured in the original space and the one computed in the low dimensional embedding.\n",
    "- MDS is closely related to PCA\n",
    "- MDS can be used also when data matrix $\\textbf{X}$ is not available and one is only provided with the matrix $R_{ij}$ of pairwise distances between points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a441929",
   "metadata": {},
   "source": [
    "## Sklearn class: [sklearn.manifold.MDS](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11b953",
   "metadata": {},
   "source": [
    "# *t-distributed Stochastic Neighbor Embedding* (t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccddde7",
   "metadata": {},
   "source": [
    "## t-SNE algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90598143",
   "metadata": {},
   "source": [
    "- t-SNE estimates the probability of each point to be a neighbor of each other point, from the distances in the high dimensional space.It then obtains the coordinates in a d-dimensional (user-defined) space in which these probabilities are as similar as possible to the one in the original space. It keeps similar instances close while dissimilar instances apart.\n",
    "- The neighborhood probabilities in the original space are represented by Gaussian joint probabilities and the probabilities in the embedded space are represented by Studentâ€™s t-distributions. The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent so that the two probabilities will be similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807c2ee2",
   "metadata": {},
   "source": [
    "## Optimization of t-SNE: perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d5021",
   "metadata": {},
   "source": [
    "- Perplexity is the number of nearest neighbors t-SNE considers when generating the conditional probabilities\n",
    "- Larger perplexities lead to more nearest neighbors and less sensitive to small structure. Conversely a lower perplexity considers a smaller number of neighbors, and thus ignores more global information in favour of the local neighborhood.\n",
    "- The recommended value is 5-50, but the perplexity should be smaller than the number of points. Generally, larger datasets require greater perplexity value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7823ae",
   "metadata": {},
   "source": [
    " ## Sklearn class: [sklearn.manifold.TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4ece7",
   "metadata": {},
   "source": [
    "## [Additional information](https://distill.pub/2016/misread-tsne/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
