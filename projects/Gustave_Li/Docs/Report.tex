\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage[caption = false]{subfig}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage{hyperref}
\usepackage[margin=2.8cm]{geometry}
\usepackage{ragged2e}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{bm}
\usepackage{xcolor}

\title{Clustering Molecular Structures for Next-generation Solar Cell Material Discovery}
\author{Gustave Li}
\date{September 5, 2021}

\begin{document}

\maketitle

\section{Abstract}
The Carotenoid-Porphyrin-\(\text{C}_{60}\) (\(\text{CPC}_{60}\)) triad molecule is a promising material in organic solar cells due to its large dipole moment when photoexcited by UV light. However, it is reported that the triad conformation largely affects its charge separation process. In this project, we aim to explore several machine learning pipelines that can cluster thousands of triad molecule based on their conformation and find the representative structures in the clusters. The original triad trajectory was first visualized, and then went through feature representation, dimensioanlity reduction, clustering and high-dimensional processes. Multiple parameters and criteria were implemented to inspected the results generated by difference processes. After comprehensive inspection of all the criteria, we picked three machine learning pipelines, from 24 different combinations, that are more suitable for triad clustering: \textbf{a)} PCA reduced \textit{xyz} data clustered by KMedoids for 6 clusters with RMSD threshold equals 0.36. \textbf{b)} PCA reduced \textit{xyz} data clustered by GMM for 7 clusters with RMSD threshold equals 0.36. \textbf{c)} kPCA (polynomial kernel) reduced \textit{xyz} data clustered by KMeans for 6 clusters with RMSD threshold equals 0.36.

\section{Introduction}
The Carotenoid-Porphyrin-\(\text{C}_{60}\) (\(\text{CPC}_{60}\)) triad molecule consists of a porphyrin covalently linked with a carotenoid and a \(\text{C}_{60}\) molecule (Figure~\ref{fig:CPC60}). Carotenoid is the excited-state electron donor and the \(\text{C}_{60}\) serves as the electron acceptor, while porphyrin acts as a bridge to separate the two parts and transfer electrons. The molecule is a mimicry of the natural photosynthetic center which utilize photons to initiate a complex series of electronic transitions to achieve a high-energy charge separated state. It absorbs UV visible light and produces a charge separated state (\(\text{CT}_{2}\)) where an electron is transferred from C to \(\text{C}_{60}\), producing a large dipole moment of 150 D. Due to its outstanding performance in photoinduced charge transfer, it has a great potential in organic solar cells.

However, Manna et al. reported that the triad spatial conformation strongly affects the process of charge separation and concluded that the linear conformations have better charge separation effeciency over the bent conformations \cite{MannaArun}. Olguin et al. further investigated the effect of structural changes on \(\text{CPC}_{60}\) charge transfer states, they summarized several factors influencing charge transfer, including donor-acceptor distance, distances and torsions between the three components \cite{OlguinMarco}. In summary, the charge transfer process in \(\text{CPC}_{60}\) is very conformation-dependent,  the molecular structure has a rather dramatic effect on the the charge transfer performance. Thus, finding the optimal structure for charge transfer is critical.

\begin{figure}[H]
    \centering
    \subfloat[Linear]{\includegraphics[width=0.4\linewidth]{projects/Gustave_Li/Docs/pics/Triad_linear.png}}
    \subfloat[Bent]{\includegraphics[width=0.3\linewidth]{projects/Gustave_Li/Docs/pics/Triad_bent.png}}
    \caption{Different conformations of \(\text{CPC}_{60}\) molecule}
    \label{fig:CPC60}
\end{figure}

Thanks to the development of computer sciences, tens of thousands of possible \(\text{CPC}_{60}\) molecules can be generated based on molecular dynamics. Considering the heavy computing load of calculation and the complexity of the \(\text{CPC}_{60}\) molecule, it is unrealistic to calculate the charge transfer rate for all the molecules. Researchers are currently working on different directions to address this issue. Brian and co-worker proposed novel formulations for calculating charge transfer rate, which reduced a maximum of 80\% of computational cost \cite{BrianDomi}. In this project, we aim to make use of machine learning to cluster the many molecules into different groups, and explore the possible machine learning pipeline that is suitable for the triad-molecule clustering scenario. By taking the cluster center as representation conformations, we expect the computational cost to decrease for a great amount while maintaining as much structural information as possible.

\section{Tools \& Platforms}

The tools and platforms used in this project are listed in Table \ref{table: tools & platforms}. Additionally, different python modules are implemented to accomplish various tasks throughout the project (see detailed information in Table \ref{table: python modules}). All the python codes and results can be found under the GitHub repository: \href{https://github.com/xiangsunlab/durf_hq/tree/master/projects/Gustave_Li}{github.com/xiangsunlab/durf\_hq/tree/master/projects/Gustave\_Li}.

\begin{table}[h!]
    \centering
    \caption{Tools and platforms used in the project} 
    \begin{tabular}{l|l}
    \hline \hline
        \textbf{Tools / Platforms} & \textbf{Description} \\
        \hline
        Python & Major programming language \\
        Markdown & Light-weight note-taking language \\
        Jupyter Notebook \& Spyder & Code experiment \& module management \\
        GitHub & Project management \\
        Overleaf & Documentation \& this report \\
        NYUSH HPC & Computing resources \\
        \hline \hline
    \end{tabular}
    \label{table: tools & platforms}
\end{table}

\begin{table}[H]
    \centering
    \caption{Python modules}
    \begin{tabular}{l|l}
    \hline \hline
        \textbf{Python modules} & \textbf{Description} \\
        \hline
        Numpy, Pandas & Data processing \\
        MDTraj\cite{MDTraj}, MDAnalysis\cite{MDAnalysis_1}\cite{MDAnalysis_2}, PyEMMA \cite{pyemma}     & Triad data reading \& analysis \\
        NGLview\cite{NGLview} & Triad molecule visualization \\
        Scikit-learn\cite{scikit-learn}, sklearn\_extra, hdbscan\cite{hdbscan} & Machine learning algorithms \\
        Matplotlib, Bokeh & Plotting \\
        \hline \hline
    \end{tabular}
    \label{table: python modules}
\end{table}

\section{Experiment Methods}

\subsection{General Workflow}
The original dataset is a molecular dynamics file containing 100,000 frames of the triad molecule. The data was first visualized to obtain basic structure and chemical features of the triad molecules. Two feature spaces were prepared for machine learning process: \textbf{a)} the xyz coordinates of each atom in the triad molecule, \textbf{b)} 8 descriptors (distances, angles, torsions) selected to represent the structure. With the feature space prepared, dimensionality reduction was applied to both spaces via different approaches and the dimensionality was reduced to 2D space for plotting and inspection conveniences. The different reduced-dimensional data were put into various clustering algorithms, which generate cluster centers (core instances) and instance labels. Quality of clustering result was evaluated with multiple criterion and some suboptimal pipelines were eliminated from the following steps. Clustering was then performed once again in the original high-dimensioanl feature space with cluster center pre-determined by the previous 2D clustering methods. Triad molecules were assigned to different clusters based on their structural similarities with core instances. High-dimensional clustering results were inspected and three machine learning methods that produces the best results were chosen.

\subsection{Descriptors} \label{sec: descriptors}
The xyz coordinates data is the simplest feature space when it comes to describing molecular structures. However, xyz coordinates suffers from several major drawbacks: First of all, the dimension of the dataset is too high. Each triad molecule has 207 atoms with 3 coordinates for each atom, so the total dimension for each entry is 621. Such large dimension may not only take up large memory for computer to store and process the data, but also increase the risk of over fitting caused by the redundant information stored in such large dimension. Secondly, xyz coordinates are orientation sensitive. While the orientation of a single molecule has very limited effect on its chemical properties, the xyz coordinates can be very different when the molecule was translated or rotated. From this point of view, the clustering algorithm will likely to over-estimate the importance of molecular orientation and position, which gives the suboptimal result.

\subsubsection{Key Atoms \& Descriptors List}
With the aim of addressing the pitfalls of xyz data space, descriptors should be concise and not orientation-sensitive. Distances between two ends of the triad describe the overall structure of the molecule, angles and dihedrals represent the bending and twisting of each part of the molecule. RMSD with reference to the bent and linear structure displays the extent of streching and bending. The full descriptors are listed in Table \ref{table: descriptors}. Considering the three-component ( Carotenoid-Porphyrin-\(\text{C}_{60}\))  nature of the molecule and its various structures, the following atoms are chosen as key atoms and all the descriptors are computed based on them: \(\text{C}_{33}\), \(\text{C}_{21}\), \(\text{C}_{61}\), \(\text{C}_{65}\), \(\text{C}_{66}\), \(\text{C}_{69}\), \(\text{C}_{89}\), \(\text{C}_{95}\), \(\text{C}_{96}\), \(\text{C}_{128}\) and \(\text{N}_{6}\) (shown in Figure \ref{fig: key atoms}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{projects/Gustave_Li/Docs/pics/key_atoms.png}
    \caption{Key atoms for descriptors}
    \label{fig: key atoms}
\end{figure} 

\begin{table}[H]
    \centering
    \caption{Definitions of geometric descriptors}
    \begin{tabular}{l|l}
    \hline \hline
       \textbf{Name}  & \textbf{Description} \\
       \hline
       EuclidianDist\_1 & The euclidian distance between \(\text{C}_{33}\) \& \(\text{C}_{128}\) \\
       Angle\_1 & The angle between atoms \(\text{C}_{33}\)-\(\text{C}_{96}\)-\(\text{C}_{128}\) \\
       Angle\_2 & The angle between atoms \(\text{C}_{33}\)-\(\text{C}_{69}\)-\(\text{C}_{96}\) \\
       Angle\_3 & The angle between atoms \(\text{C}_{69}\)-\(\text{C}_{96}\)-\(\text{C}_{128}\) \\
       Dihedral\_1 & The absolute valute of dihedral between atoms \(\text{C}_{21}\)-\(\text{C}_{61}\)-\(\text{C}_{66}\)-\(\text{C}_{65}\) \\
       Dihedral\_2 & The absolute valute of dihedral between atoms \(\text{C}_{89}\)-\(\text{N}_{6}\)-\(\text{C}_{95}\)-\(\text{C}_{96}\) \\
       RMSD\_Linear & RMSD for the conformation to the Linear triad \\
       RMSD\_Bent & RMSD for the conformation to the Bent triad \\
       \hline \hline
    \end{tabular}
    
    \label{table: descriptors}
    \vspace{1ex}
    {\noindent \justifying Note: the linear and bent conformation was determined by the overall bending angle of the triad molecule, described by the Angle\_1 descriptor. For our triad dataset, we defined the triad indexing 88213 as the linear model, which has an overall angle of \(3.10\,rad\). The bent was defined as the 29685th frame, which has an angle of \(0.36\,rad\). \par}
\end{table}

\subsection{Dimensionality Reduction}
Each of the triad molecules consist of 207 atoms, thus each molecule is represented as a data point in a 621 (\(207 \times 3\)) dimension space. In such high dimensional space, data points are far sparser than that in lower dimensional spaces, thus making it harder to find the optimal clustering result. The difficulty caused by high dimensionality is referred to as \textit{the curse of dimensionality}\cite{GeronAurelien}. Although the dimensionality was reduced to 8 by finding descriptors of the molecular structure, visualization of the 8D descriptors is still unachievable. In the face of such situation, dimensionality reduction plays its role by reducing the dimensionality of dataset significantly (to 2 or 3 for visualization) while preserving the information from original dataset\cite{GlielmoAldo}.

\subsubsection{Dimensioanlity Reduction Methods \& Hyperparameters}
Dimensionality reduction was performed on both xyz feature space and the descriptors space. Linear(PCA) and non-linear(kPCA, tSNE) dimensionality reduction methods were implemented. Hyperparameters were tuned as recorded in Table \ref{table: dimreduct}.

\begin{table}[H]
    \centering
    \caption{Dimensionality reduction method and hyperparameters}
    \begin{tabular}{p{.47\textwidth}|p{.4\textwidth}}
    \hline \hline
        \textbf{Method} & \textbf{Hyperparameters} \\
        \hline
        Principle Component Analysis (PCA) & \texttt{n\_components: 2} \\
        \hline
        kernel Principle Component Analysis (kPCA) & \texttt{n\_components: 2} \par \texttt{kernel: poly, rbf} \\
        \hline
        t-Distributed Stochastic Neighbor Embedding (t-SNE)\cite{KobakDmitry} & \texttt{n\_components: 2} \par \texttt{perplexity: 50} \par \texttt{learning\_rate: (num\_of\_data)/12} \par \texttt{init: pca} \\
        \hline \hline
    \end{tabular}
    \label{table: dimreduct}
\end{table}

\subsubsection{Dimensionality Reduction Results Inspection}
The inspection was achieved by the \texttt{pyemma.plots.plot\_density} module, which plots the dimensionality reduction results on a 2D plane and colors the data points according to the density of that region. Areas that are denser than others are likely to form meaningful clusters.

\subsection{Clustering}
Clustering divides the entire data into several distinct groups, whose elements are similar in the same group and vary between groups \cite{GlielmoAldo}. With the dimensionality reduced feature space prepared, clustering methods were implemented on the low dimensional data.  In order to explore suitable algorithms for triad molecule clustering in a vaster range, different categories of (yet common) clustering algorithms were applied, ranging from kMeans to HDBSCAN. In the meanwhile, different choices of hyperparameters were also explored and evaluated.

\subsubsection{Clustering Methods \& Hyperparameters}

\begin{table}[h!]
    \centering
    \caption{Clustering Methods \& Hyperparameters}
    \begin{tabular}{p{.28\textwidth}|p{.3\textwidth}|p{.3\textwidth}}
    \hline \hline
        \textbf{Category} & \textbf{Method} & \textbf{Hyperparameters} \\
        \hline
        \multirow{2}{.3\textwidth}{Partitioning scheme} & KMeans & \texttt{n\_clusters: 6-10} \\
        & KMedoids & \texttt{n\_clusters: 6-10} \par \texttt{init: k-medoids++} \\
        \hline
        Cluster-density based & Hierarchical density based spatial clustering of applications with noise (HDBSCAN) & \texttt{min\_cluster\_size: 10-100} \par \texttt{min\_samples: 1,10,100} \par \texttt{cluster\_selection\_epsilon: 1,10,100,500} \\
        \hline
        Probability density function & Gaussian mixture model (GMM) & \texttt{n\_components: 6-10} \par \texttt{n\_init: 10} \\
        \hline \hline
    \end{tabular}
    \label{table: clustering methods}
\end{table}

\subsubsection{Clustering Results Inspection} \label{sec: clustering inspection}
We have prepared 2 feature spaces, performed dimensionality reduction with 3 methods and implemented 4 clustering algorithms on the data, which is 24 different machine learning combinations (pipelines). For each combination, different hyperparameters also generates different results. With such diversified results, a careful selection is very necessary before moving on to high dimensioanl space. Detailed descriptions of inspection criterion are listed in Table \ref{table: clustering inspection}.

\begin{table}[H]
    \centering
    \caption{Clustering Results Inspection Methods}
    \begin{tabular}{p{.38\textwidth}|p{.6\textwidth}}
    \hline \hline
        \textbf{Methods} & \textbf{Additional notes} \\
        \hline
        Cluster map & \multirow{2}{.6\textwidth}{Provides an intuitive view of how the feature space is divided, and how instances are distributed in each cluster} \\
        Cluster population & \\
        \hline
        Akaike information criterion (AIC) & \multirow{4}{.6\textwidth}{\textbf{a)} AIC and BIC are specific for GMM models, while Inertia is only for KMeans and KMedoids models. \\ \textbf{b)} For comparison and inspection convenience, all the scoring parameters were scaled to range(0,10) before plotting. \\
        \textbf{c)} Scoring evaluates the quality of clustering on a statistical perspective.} \\
        Bayesian information criterion (BIC) & \\
        Inertia & \\
        Silhouette score & \\
          & \\
          & \\
          & \\
        \hline  
        Pairwise RMSD & \multirow{5}{.6\textwidth}{\textbf{a)} The 1000 instances nearest to each cluster center are chosen for pairwise RMSD calculation \\ \textbf{b)} RMSD and Pearson R evaluates the similarity and differences between and within clusters \\ \textbf{c)} Pearson's R are evaluated dimension-by-dimension} \\
        Pearson correlation coefficient & \\
         & \\
         & \\
         & \\
        \hline \hline
          
    \end{tabular}
    \label{table: clustering inspection}
\end{table}

\paragraph{AIC \& BIC}
AIC and BIC are criterion that select models among a finite set of models, they both introduce penalty for the number of parameters in the model to avoid overfitting \cite{wiki_BIC}. Akaike information criterion is defined as \cite{AIC}:
\begin{equation}
    \text{AIC}=-2\ln(\hat{L})+2k 
    \label{Eq: AIC}
\end{equation}
\noindent where \(k\) is the number of estimated parameters in the model, and \(\hat{L}\) is the maximum value of the likelihood function of the model.

BIC is closely related with AIC and it was derived a little bit later than AIC \cite{BIC}. Formally, Bayesian information criterion is defined as:
\begin{equation}
    \text{BIC}=-2\ln(\hat{L})+k\ln(n)
    \label{Eq: BIC}
\end{equation}
\noindent \(k\) is the number of estimated parameters in the model, \(n\) is the sample size of a certain dataset, and \(\hat{L}\) is the maximum value of the likelihood function of the model.

From the formulae of AIC and BIC, they only differ from their penalty term for model complexity, AIC has penalty form of \(2k\) and BIC is \(k\ln(n)\). The penalty term is larger in BIC than in AIC. Burnham \& Anderson pointed out that AIC can be derived in the same Bayesian framework as BIC, just by using different prior probabilities \cite{AIC_BIC_comparison}\cite{AIC_BIC_book}. Since the likelihood function is negative and the penalty term is positive, the model with lower AIC/BIC is preferred.

\paragraph{Inertia}
Inertia is a criterion that evaluates the cluster coherence generated by KMeans and KMedoids. For a clustering model with \(n\) samples and centroids \(\mu\) in set \(C\), inertia (or within-cluster sum-of-squares criterion) is defined as \cite{scikit-learn}:
\begin{equation}
    \text{Inertia}=\sum_{i=0}^{n}\min_{\mu_{j} \in C} \left( \parallel x_i - \mu_j\parallel^2 \right)
    \label{Eq: inertia}
\end{equation}
\noindent To further explain, inertia calculates the sum of distances between cluster center and each of their cluster members. When cluster number is fixed, more compact clusters will result in smaller inertia values, so the model with lower inertia is preferred. Situation becomes different when clustering results with different cluster numbers are compared. Distance between instance and cluster centers may inevitable drop with more cluster centers assigned, resulting in inertia to decrease with cluster numbers. In order to get instances fully clustered while avoid overfitting, elbow points should be identified as the optimum cluster number for a given algorithm.

\paragraph{Silhouette score}
Silhouette score measures how well an instance is inside its own cluster while away from other clusters. It considers both the distance within clusters and distance between different clusters \cite{scikit-learn}:
\begin{equation}
    s = \frac{b-a}{\max(a, b)}
    \label{Eq: Silhouette score}
\end{equation}
\noindent \(a\) is the mean distance between a sample and all other points in the same cluster, and \(b\) represents the mean distance between a sample and all other points in the next nearest cluster. A higher Silhouette Coefficient score relates to a model with better defined clusters. 

\paragraph{RMSD}
Root mean square deviation \( \rho(t) \) is a function that measures atom-wise difference and it can represent structure similarity. The RMSD as a function of time is calculated as:
\begin{equation}
    \rho(t)=\sqrt{\frac{1}{N}\sum_{i=1}^N\left(\bm{x}_i(t)-\bm{x}_i^\text{ref}\right)^2}
    \label{Eq: RMSD}
\end{equation}
\noindent where \(t\) is the time frame and \(N\) is the number of coordinates. It is the Euclidean distance in configuration space of the current configuration (possibly after optimal translation and rotation) from a reference configuration divided by \(\frac{1}{\sqrt{N}}\) \cite{scikit-learn}.

\paragraph{Pearson correlation coefficient}
Pearson correlation coefficient, or Pearson's \( \bm{r}\), gives a quantitative description of similarities between clusters. Given a paired data \(\{(\bm{x}_1, \bm{y}_1), ..., (\bm{x}_n, \bm{y}_n)\}\) consisting of \(n\) pairs, Pearson's \( \bm{r}\) is defined as:
\begin{equation}
    r_{xy} = \frac{\sum_{i=1}^n(x_i-\overline{x})(y_i-\overline{y})}{\sqrt{\sum_{i=1}^n(x_i-\overline{x})^2}\sqrt{\sum_{i=1}^n(y_i-\overline{y})^2}}
    \label{Eq: Pearson R}
\end{equation}
\noindent where \(\overline{x}, \overline{y}\) is the sample mean of \(x\) \& \(y\). Pearson's R first centers the data and returns the cosine of the angle \(\theta\) between the two observed vectors in N-dimensional space, which ranges from -1 to 1 \cite{PearsonR}. A value close to 1 means that the pair of data is very similar, and a value close to -1 shows that the pair of data points to completely opposite directions. Since the the properties of triad molecule are not sensitive to orientation, \(r_{xy}\) closer to 0 shows two clusters are different while a value approaching to -1 or 1 indicates the high similarity between clusters. 

\subsection{High-dimensioanl Clustering}
Although clustering in the 2D reduced space will also generate labels for instances, information is lost when the dimensionality is reduced, raising the risk of membership errors. To ensure the structure information be preserved to its maximum, cluster centers were obtained by clustering in 2D space and cluster members were assigned in the high-dimensional feature space, which make use of RMSD to determine membership. A triad molecule that has RMSD smaller than a given threshold (set to 0.3-0.4), with respect to a cluster center, will be regarded as a member of that cluster. Sequence of membership assignment should also be considered, since instances cannot belong to multiple clusters. The sum of absolute value of Pearson's R between a particular cluster and all other clusters is defined as the scoring-of-quality of that cluster. Lower the score, better the quality, thus the cluster with lower score will be assigned members first. Instances that belong to none of the clusters are treated as exceptions. The results are inspected by cluster populations. 

\section{Results \& Discussion}

\subsection{Dimensionality Reduction}
The density map for dimensioanlity reduction results were showed in Figure \ref{fig: dimreduct} \& Figure \ref{fig: dimreduct_xyz}. The color of datapoints represents density. Higher-density region was plotting in yellow and lower-density region plotted in dark purple.

\begin{figure}[H]
    \centering
    \subfloat[PCA]{\includegraphics[width=.45\linewidth]{projects/Gustave_Li/Docs/pics/dimreduct/pca.png}}
    \subfloat[kPCA\_poly]{\includegraphics[width=.45\linewidth]{projects/Gustave_Li/Docs/pics/dimreduct/kpca_poly.png}} \\
    \subfloat[kPCA\_rbf]{\includegraphics[width=.45\linewidth]{projects/Gustave_Li/Docs/pics/dimreduct/kpca_rbf.png}}
    \subfloat[tSNE]{\includegraphics[width=.45\linewidth]{projects/Gustave_Li/Docs/pics/dimreduct/tsne.png}}
    \caption{Dimensionality reduction on descriptors feature space}
    \label{fig: dimreduct}
\end{figure}

\begin{figure}[H]
    \centering
    \subfloat[PCA\_xyz]{\includegraphics[width=.45\linewidth]{projects/Gustave_Li/Docs/pics/dimreduct/pca_xyz.png}}
    \subfloat[kPCA\_poly\_xyz]{\includegraphics[width=.45\linewidth]{projects/Gustave_Li/Docs/pics/dimreduct/kpca_poly_xyz.png}} \\
    \subfloat[kPCA\_rbf\_xyz]{\includegraphics[width=.45\linewidth]{projects/Gustave_Li/Docs/pics/dimreduct/kpca_rbf_xyz.png}}
    \subfloat[tSNE\_xyz]{\includegraphics[width=.45\linewidth]{projects/Gustave_Li/Docs/pics/dimreduct/tsne_xyz.png}}
    \caption{Dimensionality reduction on xyz feature space}
    \label{fig: dimreduct_xyz}
\end{figure}

From the two figures, dimensioanlity reduced data was distributed in various shapes with different density. Empirically, clusters forms when high density regions were separated by low density regions (or regions without any instances). This was clearly the case for Figure \ref{fig: dimreduct_xyz}, where multiple yellow regions were surrounded by purple. However, in Figure \ref{fig: dimreduct} (a) (b) (c), the high-density region was continuous without visible low-density area in between. Data represented in Figure \ref{fig: dimreduct} (a) (b) (c) implies that all the data concentrates in one region in the reduced space, i.e., the data only forms one cluster while leaving the rest of the space sparsely filled with data. From the dimensioanlity reduction results, the xyz feature space is expected to behave better in the clustering process.

It can also be found that Figure \ref{fig: dimreduct} (a) (b) has very similar overall shape and density distribution, and analogously for \ref{fig: dimreduct_xyz} (b) (c). Since clustering algorithms works on the relative distribution of data points instead of the absolute coordinates, it is reasonable to choose one of the dimensionality reduction methods that gives similar results, for the next clustering step. Considering both the density distribution and results similarity, kPCA with rbf kernel was left out and reduced data by PCA, kPCA\_poly and tSNE on both descriptor space and xyz space will be put to clustering algorithms.

\subsection{Clustering}

\subsubsection{Cluster Map}
Cluster map gives an intuitive view of the different clusters and cluster centers, it is the most direct method to evaluate cluster quality. An optimal clustering method was expected to give a cluster map that has a moderate number of clusters with cluster member around a certain center. 
\begin{figure}[H]
    \centering
    \subfloat[Eps:1, MinClusterSize:10, MinSample:1]{\includegraphics[width=.45\linewidth]{projects/Gustave_Li/Docs/pics/Clustering/Cluster_map/hdbscan_0.png}}
    \subfloat[Eps:1, MinClusterSize:10, MinSample:1]{\includegraphics[width=.45\linewidth]{projects/Gustave_Li/Docs/pics/Clustering/Cluster_map/hdbscan_1.png}}
    \caption{HDBSCAN failure}
    \label{fig: HDBSCAN failure}
\end{figure}
After inspecting all the cases generated by HDBSCAN, it failed to meet the basic requirements of an optimum clustering results. Some typical failures are displayed in Figure \ref{fig: HDBSCAN failure}. When processing the triad data, HDBSCAN goes to two extreme: producing too many or too little clusters, and the cluster size varies greatly. HDBSCAN is ruled out from the possible methods for triad clustering.

It is assumed that the failure of HDBSCAN is caused by the nature of the triad data. The working mechanism for HDBSCAN is to find core instances and calculate neighbors of core instances, which is highly dependent to the cluster density. According to the density plots (Figure \ref{fig: dimreduct} \& Figure \ref{fig: dimreduct_xyz}), the density difference is very small between the denser region and the sparser region ((a) (b) (c)), which causes HDBSCAN to divide the whole part as one cluster. As for data generated by tSNE ((d)), the data gets too scattered and HDBSCAN assigned cluster to each of the single pieces, resulting in the algorithm to generate too many clusters.

Unlike HDBSCAN, the cluster numbers generated by KMeans, KMedoids, GMM is pre-determined, thus eliminating the probability of unreasonable cluster numbers. The results generated are quite evenly distributed in the reduced dimensioanl space (examples shown in Figure \ref{fig: clustering PCA} \& Figure \ref{fig: clustering tSNE}).

\begin{figure}[H]
    \centering
    \subfloat[KMeans]{\includegraphics[width=.3\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Cluster_map/PCA_kmeans.png}}
    \subfloat[KMedoids]{\includegraphics[width=.3\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Cluster_map/PCA_kmedoids.png}}
    \subfloat[GMM]{\includegraphics[width=.3\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Cluster_map/PCA_gmm.png}}
    \caption{Clustering results generated from PCA descriptors data, cluster num: 6}
    \label{fig: clustering PCA}
\end{figure}

\begin{figure}[H]
    \centering
    \subfloat[KMeans]{\includegraphics[width=.3\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Cluster_map/tsne_xyz_kmeans.png}}
    \subfloat[KMedoids]{\includegraphics[width=.3\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Cluster_map/tsne_xyz_kmedoids.png}}
    \subfloat[GMM]{\includegraphics[width=.3\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Cluster_map/tsne_xyz_gmm.png}}
    \caption{Clustering results generated from tSNE xyz data, cluster num: 6}
    \label{fig: clustering tSNE}
\end{figure}

\subsubsection{Cluster Population} \label{sec: lowD_pop}
Cluster population is another intuitive and critical method to evaluate clustering quality. Our aim is to find signature structures that represent its members, yet population of a cluster should be big enough to fulfill the need for representation. After inspection, it is defined that a meaningful cluster should have at least 5\% of the total population.

\begin{figure}[H]
    \centering
    \subfloat[ClusterNum:6]{\includegraphics[width=.32\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Cluster_population/PCA_xyz_kmeans_6.png}}
    \subfloat[ClusterNum:7]{\includegraphics[width=.32\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Cluster_population/PCA_xyz_kmeans_7.png}}
    \subfloat[ClusterNum:8]{\includegraphics[width=.32\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Cluster_population/PCA_xyz_kmeans_8.png}}
    \caption{Clustering population generated from PCA\_xyz\_KMeans}
    \label{fig: clustering pop}
\end{figure}

A typical example was shown in Figure \ref{fig: clustering pop}. The 5\% box was plotted in light red, and the red bars representing population should be above the 5\% line. When the cluster number was set to 6, all the cluster population exceeded 5\%, indicating a meaningful clustering result. However, when cluster number rises, one of the cluster dropped below the 5\% line and it continued to decrease as more clusters were requested. This result revealed that an existing cluster was further split and merged with other clusters when the desired cluster number increased, making the cluster population smaller thus less representative. Accordingly, setting the cluster number to 7 or 8 may suffer from overfitting.

\subsubsection{Scoring}
The scoring criteria assesses clustering performance in a statistical aspect, which took account of model complexity, cluster compactness and distance between clusters. In this project, scoring is mainly implemented to determine the better cluster numbers for each clustering algorithm. As discussed in Section \ref{sec: clustering inspection}, a lower value of AIC/BIC is expected in better models, the elbow point of inertia is preferred, and a higher value of silhouette score is expected in optimal models. It should be noticed that due to the compact nature of the triad data, the results may not look optimum in one of the single scoring criteria. Comparison and trade-off between different scoring criteria may be needed. Three representative scoring combinations are displayed and analyzed below.

\begin{figure}[H]
    \centering
    \subfloat[AIC score]{\includegraphics[width=.32\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Scoring/AIC_PCA_xyz_GMM.png}}
    \subfloat[BIC score]{\includegraphics[width=.32\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Scoring/BIC_PCA_xyz_GMM.png}}
    \subfloat[Silhouette score]{\includegraphics[width=.32\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Scoring/Silhouette_PCA_xyz_GMM.png}}
    \caption{Elbow point + maximum point}
    \label{fig: PCA_xyz_gmm}
\end{figure}

Figure \ref{fig: PCA_xyz_gmm} showed the scoring plots of GMM performing on PCA reduced xyz data. It can be observed that the AIC/BIC scoring has a decreasing trend for cluster numbers in 6-10, with an elbow point at n=7, where the graph experienced an abrupt decrease before n=7, and the decreasing trend slowed down after n=7. On the silhouette score plot, the graph reaches maximum when n=7. By reviewing AIC/BIC and silhouette score comprehensively, n=7 is the cluster number that can achieve the best result for GMM algorithm on PCA reduced xyz space, although it may not be the the optimum value with respect to the AIC/BIC scoring.

\begin{figure}[H]
    \centering
    \subfloat[Inertia]{\includegraphics[width=.5\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Scoring/Inertia_PCA_xyz_kmedoids.png}}
    \subfloat[Silhouette score]{\includegraphics[width=.5\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Scoring/Silhouette_PCA_xyz_Kmedoids.png}}
    \caption{Smooth descend + local maximum point}
    \label{fig: PCA_xyz_kmedoids}
\end{figure}

Figure \ref{fig: PCA_xyz_kmedoids} is the results for KMedoids on PCA reduced xyz space. From (a), inertia decreases in a linear pattern with the number of clusters, which makes it hard to pick out the elbow point in inertia graph. In plot (b), silhouette score dropped significantly from n=6 to n=7, then increased and remained stable. It can also be found that inertia has decreased by a considerable amount at n=8, and silhouette score hits a local maximum at the same point. Thus the possible cluster number that gives out the optimum score for PCA\_xyz\_KMedoids combination would be 6 or 8.

\begin{figure}[H]
    \centering
     \subfloat[Inertia]{\includegraphics[width=.5\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Scoring/Inertia_kPCA_poly_kmdoids.png}}
    \subfloat[Silhouette score]{\includegraphics[width=.5\textwidth]{projects/Gustave_Li/Docs/pics/Clustering/Scoring/Silhouette_kPCA_poly_kmdoids.png}}
    \caption{Smooth decrease + knee point}
    \label{fig: kPCA_poly_kmedoids}
\end{figure}

Another case is shown in Figure \ref{fig: kPCA_poly_kmedoids}, where inertia decreases smoothly across plot (a) and a knee point appears at n=8 in silhouette score plot. The line decreases slowly before a knee point and drops abruptly after that (can be comprehended as the reverse of a elbow point). At n=7 or n=8, inertia has decreased by a large amount while silhouette score maintained at an acceptable value, making them good candidates for better clustering outcomes than other cluster numbers.

According to the cases discussed above and after the inspection of all scoring results (18 combinations in total), it was found that most max/min points and knee/elbow points appeared before n=9. To further explain, the statistically optimum cluster number lies between 6 and 8. Thus, the following inspection and high-dimensioanal clustering should focus on cluster numbers between 6 and~8.

\subsubsection{Pairwise RMSD \& Pearson Correlation}
This two parameters dedicate to revealing the structure uniformity within clusters and variances between clusters. Ideally, pairwise RMSD will be very low between instances within a cluster and very high for instances between clusters. As for Pearson's R, it is expected that the correlation coefficient between clusters are close to 0 (absolute value \(< 0.3\)) in order to ensure that every cluster is unique and different from each other.

\paragraph{RMSD}
\textcolor{white}{0} 

Representative RMSD plots are displayed in Figure \ref{fig: bad_RMSD} \& Figure \ref{fig: better_RMSD}. Each "grid" was colored according to the RMSD between the x coordinates indes and the y coordinates index. The darker the color means lower the RMSD value, and vice versa. In the ideal case, the diagonal blocks from upper-left to lower-down should be colored with dark blue while the rest of the blocks are expected to have a much lighter color

In Figure \ref{fig: bad_RMSD}, the colors are very messy across the plot and no recognizable diagonal could be found. This implies that the instance structure are not consistent within a cluster, resulting in the blur diagonal. In Figure \ref{fig: bad_RMSD} (c), very few yellow color can be observed in the plot, which shows there is no significant difference in structure all across the plot. Thus the results in Figure \ref{fig: bad_RMSD} are the suboptimal RMSD results. Figure \ref{fig: better_RMSD} shows some of the better RMSD results among all of the combinations. The diagonals are quite clear and the color within the diagonal blocks are quite consistent. Bright yellow colors surround the diagonal, revealing that structures in different clusters are quite different. In the methods shown in Figure \ref{fig: better_RMSD}, triad instances are clustered by their structure quite well, according to RMSD results.

\begin{figure}[H]
    \centering
    \subfloat[PCA\_xyz\_GMM \\ n=6]{\includegraphics[width=.33\textwidth]{projects/Gustave_Li/Docs/pics/RMSD/PCA_xyz_gmm_0.png}}
    \subfloat[kPCA\_poly\_xyz\_KMeans \\ n=8]{\includegraphics[width=.33\textwidth]{projects/Gustave_Li/Docs/pics/RMSD/kPCA_poly_xyz_kmeans_2.png}}
    \subfloat[tSNE\_xyz\_KMedoids \\ n=6]{\includegraphics[width=.33\textwidth]{projects/Gustave_Li/Docs/pics/RMSD/tSNE_xyz_kmedoids_0.png}}
    \caption{Suboptimal RMSD results}
    \label{fig: bad_RMSD}
\end{figure}

\begin{figure}[h!]
    \centering
    \subfloat[tSNE\_xyz\_KMedoids \\ n=8]{\includegraphics[width=.33\textwidth]{projects/Gustave_Li/Docs/pics/RMSD/tSNE_xyz_kmedoids_2.png}}
    \subfloat[PCA\_xyz\_KMedoids \\ n=8]{\includegraphics[width=.34\textwidth]{projects/Gustave_Li/Docs/pics/RMSD/PCA_xyz_kmedoids_2.png}}
    \subfloat[kPCA\_poly\_xyz\_GMM \\ n=7]{\includegraphics[width=.33\textwidth]{projects/Gustave_Li/Docs/pics/RMSD/kPCA_poly_xyz_gmm_1.png}}
    \caption{Better RMSD results}
    \label{fig: better_RMSD}
\end{figure}



\paragraph{Pearson Correlation}
\textcolor{white}{0}

In Figure \ref{fig: Pearson_R_ncomp}, the three plot corresponds to the xyz space reduced by PCA and clustered by KMeans, setting cluster numbers to 6, 7 and 8. Only the Pearson's R for the second dimension of feature space are shown for explanation purposes. The diagonal from upper-left to lower-right has a Pearson's R of 1. This is because the diagonal represents the similarity between a cluster and itself. Since that's the same cluster, Pearson's R gives 1. For other Pearson's Rs in (a), every \(|r|\) is less than 0.3, which shows that the clusters are well separated and are not similar from each other. However, starting from n=7, \(|r|\) value greater than 0.3 begin to occur. When more clusters are assigned, clusters begin to be similar to each other. This trend reveals that one of the well-defined clusters are further separated when cluster number increases, resulting in two or more clusters in (b) \& (c) to look similar. In this case, it is pointless to assign 7 or 8 clusters for this algorithm because the new clusters are redundant.

\begin{figure}[H]
    \centering
    \subfloat[n = 6]{\includegraphics[width=.4\textwidth]{projects/Gustave_Li/Docs/pics/Pearson_R/PCA_xyz_Kmeans_0.png}}
    \subfloat[n = 7]{\includegraphics[width=.4\textwidth]{projects/Gustave_Li/Docs/pics/Pearson_R/PCA_xyz_Kmeans_1.png}} \\
    \subfloat[n = 8]{\includegraphics[width=.4\textwidth]{projects/Gustave_Li/Docs/pics/Pearson_R/PCA_xyz_Kmeans_2.png}}
    \caption{Pearson R comparison between different cluster numbers (PCA\_xyz\_KMeans)}
    \label{fig: Pearson_R_ncomp}
\end{figure}

Pearson's R can also evaluate the quality of different clustering methods and help select the optimum method for triad molecule clustering. In Figure \ref{fig: Pearson_R_methodcomp}, two Pearson's R map was shown, only with the cluster method different. In (a), the absolute value of Pearson's R from both dimension 1 and dimension 2 remains at a low value, less than 0.3. In (b), the (absolute) value reaches -0.38. Since Pearson's R closer to 0 represents a better clustering result, it can be concluded that when the reduced data comes from PCA reduced xyz space, KMedoids performs better in separating different conformations into various clusters than KMeans.

\begin{figure}[H]
    \centering
    \subfloat[KMedoids]{\includegraphics[width=.8\textwidth]{projects/Gustave_Li/Docs/pics/Pearson_R/PCA_xyz_kmedoids_1.png}} \\ 
    \subfloat[KMeans]{\includegraphics[width=.8\textwidth]{projects/Gustave_Li/Docs/pics/Pearson_R/PCA_xyz_kmeans_1_full.png}}
    \caption{Comparison between cluster methods (PCA, xyz, n=7)}
    \label{fig: Pearson_R_methodcomp}
\end{figure}

\subsection{High-dimensioanl Clustering}

In previous sections, the triad data was reduced to 2D and various clustering methods were implemented on the dimensioanlity reduced data, which gives the cluster center (core instance) of clusters. The core instances were brought to the original high dimensional space and was assigned members by RMSD thresholds. Since the results from high dimensional clustering are hard to evaluate or visualize, only the cluster population would be briefly discussed.

\subsubsection{Cluster Population}
In the discussion of population from high-dimensioanl space, we would also use the definition discussed in Section \ref{sec: lowD_pop}, regarding a cluster meaningful when it holds more than 5\% of the instances. However, since high-dimensioanl clustering was done by RMSD, instances that belongs to no cluster is inevitable ("exceptions"). When RMSD is set to a low value, it is easy to think that more instances would become exceptions, and vice versa. In this situation, the number of exceptions must be taken into consideration, since too many instances that do not have label (thus, meaningful instances too little) would result in the whole clustering result meaningless. After inspection, it is defined that the exceptional instances (with label -1) might not exceed 30\% of the entire population. When more than one RMSD value satisfies the condition, the lowest RMSD should be preferred, since lower RMSD means that the instances are more to the core instance.

\begin{figure}[H]
    \centering
    \subfloat[RMSD=0.32]{\includegraphics[width=.33\textwidth]{projects/Gustave_Li/Docs/pics/HighD_clustering/PCA_xyz_gmm_0_0.32.png}}
    \subfloat[RMSD=0.34]{\includegraphics[width=.34\textwidth]{projects/Gustave_Li/Docs/pics/HighD_clustering/PCA_xyz_gmm_0_0.34.png}}
    \subfloat[RMSD=0.40]{\includegraphics[width=.33\textwidth]{projects/Gustave_Li/Docs/pics/HighD_clustering/PCA_xyz_gmm_0_0.4.png}}
    \caption{High dimensional clustering results for PCA\_xyz\_gmm, n=6}
    \label{fig: HighD_failure}
\end{figure}

The exceptional instances are plotted in grey, and the 5\% box was plotted in red, 30\% box was plotted in light grey. For a meaningful cluster result, all the red bars should exceed the red box, while the grey bar below the gray bar. When the RMSD increases, population of the "exception" instances (grey bar) drops significantly, which is as expected. However, the drop in grey bar doesn't mean the raise of the population of all clusters (red bars), some cluster population may even drop when "exceptions" decrease, shown in Figure \ref{fig: HighD_failure}. The reason lies in the fact that several cluster centers derived from the algorithm are quite similar to each other, instances within the RMSD threshold of one core instance may also satisfy the RMSD criterion for another core instance. Since there is a sequence for membership assignment, most of the instances were assigned to the cluster with higher priority, leaving the other cluster nearly empty. Thus, this is not a satisfying result, since the core instances were repetitive, while some other signature conformations were omitted by this algorithm. Some of the combinations that satisfies both the "30\% rule" and the "5\% rule" are displayed below.

\begin{figure}[H]
    \centering
    \subfloat[PCA\_xyz\_KMedoids]{\includegraphics[width=.33\textwidth]{projects/Gustave_Li/Docs/pics/HighD_clustering/PCA_xyz_kmedoids_0_0.36.png}}
    \subfloat[tSNE\_xyz\_KMeans]{\includegraphics[width=.34\textwidth]{projects/Gustave_Li/Docs/pics/HighD_clustering/tSNE_xyz_kmeans_0_0.38.png}}
    \subfloat[PCA\_xyz\_GMM]{\includegraphics[width=.33\textwidth]{projects/Gustave_Li/Docs/pics/HighD_clustering/PCA_xyz_gmm_1_0.36.png}}
    \caption{Some high dimensional clustering results that satisfy the rules}
    \label{fig: highD_success}
\end{figure}

It should also be noted that high-dimensioanl clustering population is only one of the aspects that we evaluate clustering quality. When determining the suitable machine-learning pipelines, all of the evaluation methods mentioned above should be considered comprehensively before making the final conclusion.

\pagebreak

\section{Conclusion}

In this project, we focus on the clustering of \(\text{CPC}_{60}\) triad molecules. Basic feature representation, dimensionality reduction \& clustering methods were implemented. During the inspection process, we noticed that the descriptors feature space was not an optimal dataset since it is not effective in distinguishing the different triad conformations. We assume it is caused by the lack of representative and detailed descriptors that can depict the characteristic properties of each instance, so descriptors should be refined and more should be added to fix this problem. After rounds of inspection and selection, three machine learning pipelines were picked from the pool to be more suitable for triad molecule clustering. Detailed parameters were listed in Table \ref{tab: final results}.

\begin{table}[H]
    \centering
    \caption{Machine learning pipelines suitable for triad-molecule clustering}
    \begin{tabular}{c|c|c|c|c}
    \hline \hline
        \textbf{Feature space} & \textbf{DimRed} & \textbf{Clustering} & \textbf{Cluster number} & \textbf{RMSD threshold} \\
        \hline
        \textit{xyz} space & PCA & KMedoids & 6 & 0.36 \\
        \textit{xyz} space & PCA & GMM & 7 & 0.36 \\
        \textit{xyz} space & kPCA\_poly & KMeans & 6 & 0.36 \\
        \hline \hline
    \end{tabular}
    \label{tab: final results}
\end{table}

\section{Future Works}
The project was re-visited after it was almost done and we summarized several points that could be further accomplished. Firstly, the feature space selection was not very successful in this project. Although we eventually chose the original \textit{xyz} coordinate instead of the descriptors space, \textit{xyz} coordinates suffers a lot of drawbacks discussed in previous sections (Section \ref{sec: descriptors}). We should try to refine the descriptors feature space in order to obtain better results. Secondly, the machine learning methods implemented in this project were very basic and common yet they were not designed specifically for molecular dynamics data. Future work could focus on more advanced and developed machine learning methods targeting MD data (vsCNN \cite{vsCNN}, InfleCS \cite{InfleCS}) in order to get more accurate clustering results. Finally, as for the inspection part, several landmark structures can be put into real test to inspect the similarities and differences regarding their chemical properties, instead of solely dealing with statistical and structural properties.

\pagebreak
\bibliographystyle{unsrt}
\bibliography{references.bib}

\end{document}